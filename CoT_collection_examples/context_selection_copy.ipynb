{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/roberta-large-nli-stsb-mean-tokens')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/roberta-large-nli-stsb-mean-tokens').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_pool_path = \"\"\n",
    "cot_remain_pool_path = \"\"\n",
    "df_finished = pd.read_table(cot_pool_path, sep='\\t', encoding='utf-8')\n",
    "df_unfinished = pd.read_table(cot_remain_pool_path, sep='\\t', encoding='utf-8')\n",
    "questions_finished = list(df_finished['question'])\n",
    "questions_unfinished = list(df_unfinished['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_finished = []\n",
    "for i in range(0, len(questions_finished), 64):\n",
    "    batch = questions_finished[i: i + 64]\n",
    "    inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "    inputs = inputs.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    sentence_embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
    "    embed_finished.append(sentence_embeddings)\n",
    "embed_finished = torch.concat(embed_finished, axis=0)\n",
    "\n",
    "embed_unfinished = []\n",
    "for i in range(0, len(questions_unfinished), 64):\n",
    "    batch = questions_unfinished[i: i + 64]\n",
    "    inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "    inputs = inputs.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    sentence_embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
    "    embed_unfinished.append(sentence_embeddings)\n",
    "embed_unfinished = torch.concat(embed_unfinished, axis=0)\n",
    "# print(embed_finished.shape, embed_unfinished.shape)\n",
    "# cosine similarity\n",
    "similarity = []\n",
    "for i in tqdm(range(0, embed_finished.size(0), 128)):\n",
    "    sim = torch.cosine_similarity(embed_unfinished.unsqueeze(1), embed_finished[i: i + 128].unsqueeze(0), dim=-1).cpu()\n",
    "    similarity.append(sim)\n",
    "similarity = torch.concat(similarity, axis=-1)\n",
    "# print(similarity.shape)\n",
    "indexs = torch.argmax(similarity, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \\\n",
    "'''Please follow the example to generate your answers (For actions, only use 'Question', 'Multi_Answer_Question' and 'Finish'. Only use special questions when doing action 'Question' and 'Multi_Answer_Question'. Generate your inference steps until action 'Finish'):\\n\\n{demonstration}\\n\\nQuestion {question}\\nHint {hint}'''\n",
    "prompt_demon = '''Question {question}\\nHint {hint}\\n{cot}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = []\n",
    "for i, q in enumerate(questions_unfinished):\n",
    "    most_sim = int(indexs[i])\n",
    "    dict_most_sim = dict(df_finished.loc[most_sim])\n",
    "    ground_truth = eval(dict_most_sim['ground_truth'])\n",
    "    this_hint = {\"answer\": ground_truth['ground_answer'], \"composition_answer\": ground_truth['composition_answer']}\n",
    "    this_prompt = prompt_demon.format(question=dict_most_sim['question'], hint=this_hint, cot=dict_most_sim['gpt_out'])\n",
    "    all_info = dict(df_unfinished.loc[i])\n",
    "    hint = {\"answer\": eval(all_info['ground_answer']), \"composition_answer\": eval(all_info['compositional_answer'])}\n",
    "    instance = prompt.format(demonstration=this_prompt, question=q, hint=hint)\n",
    "    instance = instance.replace(\"\\\\n\", \"\\n\")\n",
    "    instance = instance.replace(\"\\n\", \"\\\\n\")\n",
    "    instances.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '''Your Save Path'''\n",
    "df_to_gpt = pd.DataFrame(columns=['question'])\n",
    "df_to_gpt['question'] = instances\n",
    "df_to_gpt.to_csv(save_path, index=False, encoding='ut-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
